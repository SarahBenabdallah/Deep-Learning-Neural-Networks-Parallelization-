{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarahBenabdallah/Deep-Learning-Neural-Networks-Parallelization-/blob/main/PFA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQd4Ykb04jCM"
      },
      "source": [
        "**Installations and Imports**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy1V9T8-5GZ_"
      },
      "source": [
        "In addition to the pre-installed packages like numpy, pandas, matplotlib, keras, tensorflow, we'll install concise, a keras extension for regulatory genomics https://www.cmm.in.tum.de/public/docs/concise/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEWve8i0vzfX",
        "outputId": "748e2964-3d7e-4e95-dad5-64b476c316da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.7/dist-packages (2.9.1)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (4.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (57.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.46.1)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Using cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.14.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (14.0.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Collecting numpy>=1.20\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 16.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (21.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.26.0)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Using cached tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Using cached tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.37.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-gpu) (3.0.9)\n",
            "Installing collected packages: numpy, tensorflow-estimator, tensorboard, keras\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 1.15.5 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.21.6 which is incompatible.\n",
            "tensorflow 1.15.5 requires tensorboard<1.16.0,>=1.15.0, but you have tensorboard 2.9.0 which is incompatible.\n",
            "tensorflow 1.15.5 requires tensorflow-estimator==1.15.1, but you have tensorflow-estimator 2.9.0 which is incompatible.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "pysster 1.2.2 requires keras<2.3.0, but you have keras 2.9.0 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.9.0 numpy-1.21.6 tensorboard-2.9.0 tensorflow-estimator-2.9.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.9.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.46.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (4.2.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pysster in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from pysster) (2.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pysster) (3.2.2)\n",
            "Collecting keras<2.3.0\n",
            "  Using cached Keras-2.2.5-py2.py3-none-any.whl (336 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pysster) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pysster) (1.21.6)\n",
            "Requirement already satisfied: logging-exceptions in /usr/local/lib/python3.7/dist-packages (from pysster) (0.1.8)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pysster) (1.0.2)\n",
            "Requirement already satisfied: tensorflow<2.0 in /usr/local/lib/python3.7/dist-packages (from pysster) (1.15.5)\n",
            "Requirement already satisfied: fastcluster in /usr/local/lib/python3.7/dist-packages (from pysster) (1.2.6)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pysster) (0.11.2)\n",
            "Requirement already satisfied: forgi in /usr/local/lib/python3.7/dist-packages (from pysster) (2.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras<2.3.0->pysster) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras<2.3.0->pysster) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras<2.3.0->pysster) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras<2.3.0->pysster) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras<2.3.0->pysster) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0->pysster) (1.46.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0->pysster) (0.37.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0->pysster) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0->pysster) (0.2.0)\n",
            "Collecting numpy>=1.14.0\n",
            "  Using cached numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Using cached tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0->pysster) (1.14.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Using cached tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0->pysster) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0->pysster) (0.2.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0->pysster) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0->pysster) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0->pysster) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0->pysster) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0->pysster) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0->pysster) (3.3.7)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0->pysster) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0->pysster) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0->pysster) (3.8.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from forgi->pysster) (0.29.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from forgi->pysster) (0.16.0)\n",
            "Requirement already satisfied: pandas>=0.20 in /usr/local/lib/python3.7/dist-packages (from forgi->pysster) (1.3.5)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.7/dist-packages (from forgi->pysster) (1.79)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from forgi->pysster) (2.6.3)\n",
            "Requirement already satisfied: appdirs>=1.4 in /usr/local/lib/python3.7/dist-packages (from forgi->pysster) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20->forgi->pysster) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20->forgi->pysster) (2022.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pysster) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pysster) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pysster) (0.11.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pysster) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pysster) (3.1.0)\n",
            "Installing collected packages: numpy, tensorflow-estimator, tensorboard, keras\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.0\n",
            "    Uninstalling tensorboard-2.9.0:\n",
            "      Successfully uninstalled tensorboard-2.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "tensorflow-gpu 2.9.1 requires keras<2.10.0,>=2.9.0rc0, but you have keras 2.2.5 which is incompatible.\n",
            "tensorflow-gpu 2.9.1 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow-gpu 2.9.1 requires tensorboard<2.10,>=2.9, but you have tensorboard 1.15.0 which is incompatible.\n",
            "tensorflow-gpu 2.9.1 requires tensorflow-estimator<2.10.0,>=2.9.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.5 which is incompatible.\n",
            "jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.8 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.2.5 numpy-1.18.5 tensorboard-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install tensorflow-gpu\n",
        "!pip install -U tensorboard\n",
        "!pip install pysster\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pysster.One_Hot_Encoder import One_Hot_Encoder\n",
        "one = One_Hot_Encoder('ACGT')\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6SP4zbUreD4k"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffzvpsUp6exM"
      },
      "source": [
        "**Get Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "562ghqVB9-QE"
      },
      "source": [
        "We are going to use simulated data of 10,000 500 bp long sequences with the positive set containing an instance of the TAL1 motif: \n",
        "TAL1 known4 and the negative set will be random sequences. \n",
        "The data were simulated using simDNA https://github.com/kundajelab/simdna by Johnny Israeli\n",
        "and were deposited to https://github.com/kundajelab/dragonn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Lzoak9pMPEZ0",
        "outputId": "f4879c96-a94d-40bf-9792-1cd7f8d92f91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-05-31 09:49:04--  https://github.com/kundajelab/dragonn/raw/master/paper_supplement/simulation_data/GC_fraction0.4motif_nameTAL1_known4num_neg10000num_pos10000seq_length500.npz\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/kundajelab/dragonn/master/paper_supplement/simulation_data/GC_fraction0.4motif_nameTAL1_known4num_neg10000num_pos10000seq_length500.npz [following]\n",
            "--2022-05-31 09:49:05--  https://raw.githubusercontent.com/kundajelab/dragonn/master/paper_supplement/simulation_data/GC_fraction0.4motif_nameTAL1_known4num_neg10000num_pos10000seq_length500.npz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6015773 (5.7M) [application/octet-stream]\n",
            "Saving to: ‘GC_fraction0.4motif_nameTAL1_known4num_neg10000num_pos10000seq_length500.npz.1’\n",
            "\n",
            "GC_fraction0.4motif 100%[===================>]   5.74M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-05-31 09:49:05 (172 MB/s) - ‘GC_fraction0.4motif_nameTAL1_known4num_neg10000num_pos10000seq_length500.npz.1’ saved [6015773/6015773]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#get the dataset npz version\n",
        "!wget 'https://github.com/kundajelab/dragonn/raw/master/paper_supplement/simulation_data/GC_fraction0.4motif_nameTAL1_known4num_neg10000num_pos10000seq_length500.npz'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daFAwLCI-Z_Y"
      },
      "source": [
        "**Load the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dAbaXiW6PGWV"
      },
      "outputs": [],
      "source": [
        "def load_simulated_data(path):\n",
        "    \"\"\"Load the simulated dataset\n",
        "    \n",
        "    Args:\n",
        "      path: path to the .npz file c\n",
        "    \"\"\"\n",
        "    data = np.load(path)\n",
        "\n",
        "    x_train = data[\"X_train\"].squeeze(1).swapaxes(1,2)\n",
        "    x_valid = data[\"X_valid\"].squeeze(1).swapaxes(1,2)\n",
        "    y_train = data['y_train']\n",
        "    y_valid = data['y_valid']\n",
        "    return (x_train, y_train), (x_valid, y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wlMNF5FRj0aD",
        "outputId": "a74f799b-f8f8-4ae3-def4-433dfc2a2a65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_test.csv\n",
            "df_train.csv\n",
            "GC_fraction0.4motif_nameTAL1_known4num_neg10000num_pos10000seq_length500.npz\n",
            "GC_fraction0.4motif_nameTAL1_known4num_neg10000num_pos10000seq_length500.npz.1\n",
            "\u001b[0m\u001b[01;34mlogs\u001b[0m/\n",
            "\u001b[01;34msample_data\u001b[0m/\n",
            "x_test.csv\n",
            "x_train.csv\n",
            "y_test.csv\n",
            "y_train.csv\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL9cCqT6_ZnY"
      },
      "source": [
        "and load the first file. The data is splitted into training dataset, on wchih the model will be fitted, and a test dataset on which its performance will be assessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v4WmMe0RPNg-"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = load_simulated_data(\"GC_fraction0.4motif_nameTAL1_known4num_neg10000num_pos10000seq_length500.npz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3-Jsi5d_oNU"
      },
      "source": [
        "**Print data shapes** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vj_UASl4oiMz",
        "outputId": "a6acb576-18da-4470-a202-38a71b6aeeb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape :  (12800, 500, 4)\n",
            "y_train shape :  (12800, 1)\n",
            "x_test shape :  (3200, 500, 4)\n",
            "y_test shape :  (3200, 1)\n"
          ]
        }
      ],
      "source": [
        "print('x_train shape : ' ,x_train.shape)\n",
        "print('y_train shape : ' ,y_train.shape)\n",
        "print('x_test shape : ' ,x_test.shape)\n",
        "print('y_test shape : ' ,y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQHGRv-P_2R4"
      },
      "source": [
        "As you can see, there are 12.8k training examples and 3.2k test examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL-0jkRI_59o"
      },
      "source": [
        "**Transform one hot encoded sequences to DNA strings**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cXKnzdcdPQvv"
      },
      "outputs": [],
      "source": [
        "x_train_encoded = [one.decode(x) for x in x_train]\n",
        "x_test_encoded = [one.decode(x) for x in x_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUOmZNGDBBsA"
      },
      "source": [
        "**Transform numpy train data to csv files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dKoHs7tcY7_u"
      },
      "outputs": [],
      "source": [
        "x_train_df = pd.DataFrame(x_train_encoded)\n",
        "x_train_df.to_csv('x_train.csv')\n",
        "y_train_df = pd.DataFrame(y_train)\n",
        "y_train_df.to_csv('y_train.csv')\n",
        "df_train = pd.concat([x_train_df, y_train_df], axis= 1)\n",
        "df_train.to_csv('df_train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYH0nW50BaQZ"
      },
      "source": [
        "**Transform numpy test data to csv files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cYDfM82zDQr"
      },
      "outputs": [],
      "source": [
        "x_test_df = pd.DataFrame(x_test_encoded)\n",
        "x_test_df.to_csv('x_test.csv')\n",
        "y_test_df = pd.DataFrame(y_test)\n",
        "y_test_df.to_csv('y_test.csv')\n",
        "df_test = pd.concat([x_test_df, y_test_df], axis= 1)\n",
        "df_test.to_csv('df_test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zirRKuwjBhGo"
      },
      "source": [
        "**Visualize train DF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbJ_n519vi20"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIXBxOqsBmPT"
      },
      "source": [
        "**Visualize test DF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjiOwAxLzchG"
      },
      "outputs": [],
      "source": [
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv3y54ipBq30"
      },
      "source": [
        "**Download train data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Geg5MQxyPsXN"
      },
      "outputs": [],
      "source": [
        "files.download('x_train.csv')\n",
        "files.download('y_train.csv')\n",
        "files.download('df_train.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suC8ykwhBwid"
      },
      "source": [
        "**Download test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q90NwMAozlY7"
      },
      "outputs": [],
      "source": [
        "files.download('x_test.csv')\n",
        "files.download('y_test.csv')\n",
        "files.download('df_test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fam9k48tB8mK"
      },
      "source": [
        "**Analyze Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-FU4eui1zD_"
      },
      "outputs": [],
      "source": [
        "y_train.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ri6aQKd19n_"
      },
      "outputs": [],
      "source": [
        "y_test.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-hEc3u_CDz8"
      },
      "source": [
        "As the means are close to 0.5, there are balanced classes, i.e. roughly the same number of positive and negative instances. This ratio is the same for training and testing. This is an ideal situation for training a classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYcUnaCo2Ar_"
      },
      "outputs": [],
      "source": [
        "x_train[0][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D0FjhicCIiC"
      },
      "source": [
        "As you can see the input is the one-hot-encoded DNA sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HML2PpzTC7WJ"
      },
      "source": [
        "**Define the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjQVdRtlER2I"
      },
      "source": [
        "We'll use Keras (https://keras.io), a popular deep learning library wrapping frameworks like TensorFlow to define and train a neural network. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq0E6KTT2Bzn"
      },
      "outputs": [],
      "source": [
        "#IMPORTS:\n",
        "from keras.models import Sequential\n",
        "import keras.layers as kl\n",
        "from keras.callbacks import EarlyStopping, History, TensorBoard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNlt_MIgDUYR"
      },
      "source": [
        "Define the model architecture.\n",
        "It can be simply define as a sequence of layers using keras.models.Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqtz4X2dHNoU"
      },
      "source": [
        "We will now implement a neural network consisting of 16 convolutional filters whose outputs are transfomed by the ReLU activation function ( ReLU(x)=max(0,x) ) . On each of the 16 vectors, we keep the maximal value (\"max pooling\") giving a us 16-long vector. We then perform a linear transformation of this 16-long vector into a single scalar (\"Dense layer\") which we map to the  [0,1]  interval with sigmoid activation function  sigm(x)=11+exp(−x)  . The interpratation of the output is the probability that the sequence is bound by the transcription factor TAL1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbST8bSN2Qdf"
      },
      "outputs": [],
      "source": [
        "model = Sequential([kl.Conv1D(filters=16, kernel_size=15, activation='relu', input_shape=x_train.shape[1:]),\n",
        "                    kl.GlobalMaxPooling1D(),\n",
        "                    kl.Dense(units=1),\n",
        "                    kl.Activation('sigmoid')\n",
        "                   ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J9WBJ3IDqgj"
      },
      "source": [
        "Compile the model. We need to specify which optimizer do we use, what is the loss function and what additional metrics we want to use. In our case we use the adam optimizer, the binary crossentropy as a loss function and accuracy as a metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zoyrdSq2YI2"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvTPHqCiEDEI"
      },
      "source": [
        "Visualize model layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfjTCmcE2eAv"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnUrO32BH5or"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p3RLlcFH_9A"
      },
      "source": [
        " Making sure our results are reproducible.The model is namely initialized using random weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGOBztlM2ifo"
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rPRQQwwJ2qMS",
        "outputId": "a73f1db8-b49e-477b-b353-9ad6fe2350e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 10240 samples, validate on 2560 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/callbacks.py:1068: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/100\n",
            "10240/10240 [==============================] - 3s 276us/step - loss: 0.6977 - acc: 0.4961 - val_loss: 0.6919 - val_acc: 0.5305\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Epoch 2/100\n",
            "10240/10240 [==============================] - 3s 258us/step - loss: 0.6923 - acc: 0.5144 - val_loss: 0.6901 - val_acc: 0.5363\n",
            "Epoch 3/100\n",
            "10240/10240 [==============================] - 5s 472us/step - loss: 0.6890 - acc: 0.5473 - val_loss: 0.6866 - val_acc: 0.5719\n",
            "Epoch 4/100\n",
            "10240/10240 [==============================] - 4s 430us/step - loss: 0.6839 - acc: 0.5685 - val_loss: 0.6805 - val_acc: 0.5973\n",
            "Epoch 5/100\n",
            "10240/10240 [==============================] - 2s 241us/step - loss: 0.6760 - acc: 0.6232 - val_loss: 0.6718 - val_acc: 0.6734\n",
            "Epoch 6/100\n",
            "10240/10240 [==============================] - 2s 242us/step - loss: 0.6651 - acc: 0.6939 - val_loss: 0.6612 - val_acc: 0.7281\n",
            "Epoch 7/100\n",
            "10240/10240 [==============================] - 2s 242us/step - loss: 0.6518 - acc: 0.7394 - val_loss: 0.6468 - val_acc: 0.7703\n",
            "Epoch 8/100\n",
            "10240/10240 [==============================] - 2s 241us/step - loss: 0.6356 - acc: 0.7861 - val_loss: 0.6297 - val_acc: 0.8043\n",
            "Epoch 9/100\n",
            "10240/10240 [==============================] - 2s 241us/step - loss: 0.6177 - acc: 0.8070 - val_loss: 0.6123 - val_acc: 0.8203\n",
            "Epoch 10/100\n",
            "10240/10240 [==============================] - 2s 239us/step - loss: 0.6000 - acc: 0.8303 - val_loss: 0.5953 - val_acc: 0.8355\n",
            "Epoch 11/100\n",
            "10240/10240 [==============================] - 2s 238us/step - loss: 0.5824 - acc: 0.8444 - val_loss: 0.5779 - val_acc: 0.8438\n",
            "Epoch 12/100\n",
            "10240/10240 [==============================] - 2s 238us/step - loss: 0.5633 - acc: 0.8602 - val_loss: 0.5587 - val_acc: 0.8562\n",
            "Epoch 13/100\n",
            "10240/10240 [==============================] - 2s 244us/step - loss: 0.5428 - acc: 0.8664 - val_loss: 0.5373 - val_acc: 0.8660\n",
            "Epoch 14/100\n",
            "10240/10240 [==============================] - 2s 242us/step - loss: 0.5206 - acc: 0.8760 - val_loss: 0.5151 - val_acc: 0.8715\n",
            "Epoch 15/100\n",
            "10240/10240 [==============================] - 2s 242us/step - loss: 0.4971 - acc: 0.8817 - val_loss: 0.4922 - val_acc: 0.8801\n",
            "Epoch 16/100\n",
            "10240/10240 [==============================] - 2s 241us/step - loss: 0.4726 - acc: 0.8893 - val_loss: 0.4683 - val_acc: 0.8895\n",
            "Epoch 17/100\n",
            "10240/10240 [==============================] - 2s 241us/step - loss: 0.4485 - acc: 0.8944 - val_loss: 0.4461 - val_acc: 0.8914\n",
            "Epoch 18/100\n",
            "10240/10240 [==============================] - 3s 244us/step - loss: 0.4262 - acc: 0.8972 - val_loss: 0.4245 - val_acc: 0.8973\n",
            "Epoch 19/100\n",
            "10240/10240 [==============================] - 2s 239us/step - loss: 0.4049 - acc: 0.9013 - val_loss: 0.4053 - val_acc: 0.8988\n",
            "Epoch 20/100\n",
            "10240/10240 [==============================] - 2s 241us/step - loss: 0.3862 - acc: 0.9030 - val_loss: 0.3879 - val_acc: 0.9035\n",
            "Epoch 21/100\n",
            "10240/10240 [==============================] - 2s 238us/step - loss: 0.3681 - acc: 0.9083 - val_loss: 0.3706 - val_acc: 0.9090\n",
            "Epoch 22/100\n",
            "10240/10240 [==============================] - 2s 242us/step - loss: 0.3514 - acc: 0.9126 - val_loss: 0.3552 - val_acc: 0.9117\n",
            "Epoch 23/100\n",
            "10240/10240 [==============================] - 2s 236us/step - loss: 0.3360 - acc: 0.9149 - val_loss: 0.3411 - val_acc: 0.9168\n",
            "Epoch 24/100\n",
            "10240/10240 [==============================] - 2s 237us/step - loss: 0.3226 - acc: 0.9199 - val_loss: 0.3289 - val_acc: 0.9195\n",
            "Epoch 25/100\n",
            "10240/10240 [==============================] - 2s 239us/step - loss: 0.3097 - acc: 0.9233 - val_loss: 0.3177 - val_acc: 0.9227\n",
            "Epoch 26/100\n",
            "10240/10240 [==============================] - 2s 242us/step - loss: 0.2986 - acc: 0.9274 - val_loss: 0.3084 - val_acc: 0.9203\n",
            "Epoch 27/100\n",
            "10240/10240 [==============================] - 2s 239us/step - loss: 0.2885 - acc: 0.9293 - val_loss: 0.2990 - val_acc: 0.9289\n",
            "Epoch 28/100\n",
            "10240/10240 [==============================] - 2s 244us/step - loss: 0.2790 - acc: 0.9317 - val_loss: 0.2916 - val_acc: 0.9273\n",
            "Epoch 29/100\n",
            "10240/10240 [==============================] - 2s 242us/step - loss: 0.2709 - acc: 0.9329 - val_loss: 0.2838 - val_acc: 0.9301\n",
            "Epoch 30/100\n",
            "10240/10240 [==============================] - 2s 242us/step - loss: 0.2629 - acc: 0.9361 - val_loss: 0.2760 - val_acc: 0.9309\n",
            "Epoch 31/100\n",
            "10240/10240 [==============================] - 2s 241us/step - loss: 0.2557 - acc: 0.9375 - val_loss: 0.2696 - val_acc: 0.9348\n",
            "Epoch 32/100\n",
            "10240/10240 [==============================] - 3s 274us/step - loss: 0.2485 - acc: 0.9390 - val_loss: 0.2640 - val_acc: 0.9359\n",
            "Epoch 33/100\n",
            "10240/10240 [==============================] - 2s 236us/step - loss: 0.2425 - acc: 0.9406 - val_loss: 0.2585 - val_acc: 0.9371\n",
            "Epoch 34/100\n",
            "10240/10240 [==============================] - 3s 272us/step - loss: 0.2362 - acc: 0.9423 - val_loss: 0.2536 - val_acc: 0.9391\n",
            "Epoch 35/100\n",
            "10240/10240 [==============================] - 2s 237us/step - loss: 0.2309 - acc: 0.9440 - val_loss: 0.2498 - val_acc: 0.9387\n",
            "Epoch 36/100\n",
            "10240/10240 [==============================] - 2s 232us/step - loss: 0.2263 - acc: 0.9444 - val_loss: 0.2439 - val_acc: 0.9395\n",
            "Epoch 37/100\n",
            "10240/10240 [==============================] - 3s 326us/step - loss: 0.2211 - acc: 0.9458 - val_loss: 0.2400 - val_acc: 0.9406\n",
            "Epoch 38/100\n",
            "10240/10240 [==============================] - 2s 236us/step - loss: 0.2166 - acc: 0.9461 - val_loss: 0.2363 - val_acc: 0.9414\n",
            "Epoch 39/100\n",
            "10240/10240 [==============================] - 2s 237us/step - loss: 0.2124 - acc: 0.9471 - val_loss: 0.2330 - val_acc: 0.9422\n",
            "Epoch 40/100\n",
            "10240/10240 [==============================] - 2s 240us/step - loss: 0.2083 - acc: 0.9488 - val_loss: 0.2302 - val_acc: 0.9441\n",
            "Epoch 41/100\n",
            "10240/10240 [==============================] - 2s 238us/step - loss: 0.2046 - acc: 0.9496 - val_loss: 0.2268 - val_acc: 0.9465\n",
            "Epoch 42/100\n",
            "10240/10240 [==============================] - 2s 240us/step - loss: 0.2015 - acc: 0.9505 - val_loss: 0.2250 - val_acc: 0.9449\n",
            "Epoch 43/100\n",
            "10240/10240 [==============================] - 2s 240us/step - loss: 0.1978 - acc: 0.9521 - val_loss: 0.2218 - val_acc: 0.9441\n",
            "Epoch 44/100\n",
            "10240/10240 [==============================] - 2s 237us/step - loss: 0.1948 - acc: 0.9519 - val_loss: 0.2190 - val_acc: 0.9449\n",
            "Epoch 45/100\n",
            "10240/10240 [==============================] - 2s 238us/step - loss: 0.1921 - acc: 0.9524 - val_loss: 0.2167 - val_acc: 0.9473\n",
            "Epoch 46/100\n",
            "10240/10240 [==============================] - 2s 237us/step - loss: 0.1888 - acc: 0.9537 - val_loss: 0.2155 - val_acc: 0.9469\n",
            "Epoch 47/100\n",
            "10240/10240 [==============================] - 2s 237us/step - loss: 0.1864 - acc: 0.9542 - val_loss: 0.2129 - val_acc: 0.9484\n",
            "Epoch 48/100\n",
            "10240/10240 [==============================] - 2s 238us/step - loss: 0.1840 - acc: 0.9554 - val_loss: 0.2119 - val_acc: 0.9477\n",
            "Epoch 49/100\n",
            "10240/10240 [==============================] - 2s 241us/step - loss: 0.1813 - acc: 0.9561 - val_loss: 0.2081 - val_acc: 0.9480\n",
            "Epoch 50/100\n",
            "10240/10240 [==============================] - 2s 242us/step - loss: 0.1782 - acc: 0.9557 - val_loss: 0.2059 - val_acc: 0.9488\n",
            "Epoch 51/100\n",
            "10240/10240 [==============================] - 3s 245us/step - loss: 0.1762 - acc: 0.9570 - val_loss: 0.2037 - val_acc: 0.9492\n",
            "Epoch 52/100\n",
            "10240/10240 [==============================] - 2s 241us/step - loss: 0.1731 - acc: 0.9574 - val_loss: 0.2012 - val_acc: 0.9500\n",
            "Epoch 53/100\n",
            "10240/10240 [==============================] - 2s 239us/step - loss: 0.1705 - acc: 0.9589 - val_loss: 0.1989 - val_acc: 0.9512\n",
            "Epoch 54/100\n",
            "10240/10240 [==============================] - 2s 238us/step - loss: 0.1685 - acc: 0.9594 - val_loss: 0.1970 - val_acc: 0.9508\n",
            "Epoch 55/100\n",
            "10240/10240 [==============================] - 3s 246us/step - loss: 0.1661 - acc: 0.9600 - val_loss: 0.1954 - val_acc: 0.9508\n",
            "Epoch 56/100\n",
            "10240/10240 [==============================] - 2s 241us/step - loss: 0.1644 - acc: 0.9607 - val_loss: 0.1945 - val_acc: 0.9535\n",
            "Epoch 57/100\n",
            "10240/10240 [==============================] - 2s 235us/step - loss: 0.1622 - acc: 0.9608 - val_loss: 0.1925 - val_acc: 0.9516\n",
            "Epoch 58/100\n",
            "10240/10240 [==============================] - 2s 244us/step - loss: 0.1603 - acc: 0.9614 - val_loss: 0.1912 - val_acc: 0.9543\n",
            "Epoch 59/100\n",
            "10240/10240 [==============================] - 2s 240us/step - loss: 0.1587 - acc: 0.9615 - val_loss: 0.1899 - val_acc: 0.9539\n",
            "Epoch 60/100\n",
            "10240/10240 [==============================] - 2s 243us/step - loss: 0.1570 - acc: 0.9625 - val_loss: 0.1885 - val_acc: 0.9531\n",
            "Epoch 61/100\n",
            "10240/10240 [==============================] - 2s 234us/step - loss: 0.1556 - acc: 0.9627 - val_loss: 0.1874 - val_acc: 0.9531\n",
            "Epoch 62/100\n",
            "10240/10240 [==============================] - 2s 238us/step - loss: 0.1539 - acc: 0.9629 - val_loss: 0.1867 - val_acc: 0.9547\n",
            "Epoch 63/100\n",
            "10240/10240 [==============================] - 2s 238us/step - loss: 0.1528 - acc: 0.9630 - val_loss: 0.1855 - val_acc: 0.9535\n",
            "Epoch 64/100\n",
            "10240/10240 [==============================] - 2s 230us/step - loss: 0.1515 - acc: 0.9636 - val_loss: 0.1846 - val_acc: 0.9539\n",
            "Epoch 65/100\n",
            "10240/10240 [==============================] - 2s 232us/step - loss: 0.1500 - acc: 0.9637 - val_loss: 0.1839 - val_acc: 0.9551\n",
            "Epoch 66/100\n",
            "10240/10240 [==============================] - 2s 231us/step - loss: 0.1489 - acc: 0.9638 - val_loss: 0.1829 - val_acc: 0.9543\n",
            "Epoch 67/100\n",
            "10240/10240 [==============================] - 2s 241us/step - loss: 0.1477 - acc: 0.9638 - val_loss: 0.1823 - val_acc: 0.9555\n",
            "Epoch 68/100\n",
            "10240/10240 [==============================] - 3s 254us/step - loss: 0.1467 - acc: 0.9646 - val_loss: 0.1818 - val_acc: 0.9539\n",
            "Epoch 69/100\n",
            "10240/10240 [==============================] - 2s 235us/step - loss: 0.1461 - acc: 0.9645 - val_loss: 0.1811 - val_acc: 0.9531\n",
            "Epoch 70/100\n",
            "10240/10240 [==============================] - 3s 252us/step - loss: 0.1447 - acc: 0.9648 - val_loss: 0.1803 - val_acc: 0.9543\n",
            "Epoch 71/100\n",
            "10240/10240 [==============================] - 3s 252us/step - loss: 0.1434 - acc: 0.9649 - val_loss: 0.1797 - val_acc: 0.9547\n",
            "Epoch 72/100\n",
            "10240/10240 [==============================] - 3s 246us/step - loss: 0.1424 - acc: 0.9647 - val_loss: 0.1799 - val_acc: 0.9559\n",
            "Epoch 73/100\n",
            "10240/10240 [==============================] - 3s 252us/step - loss: 0.1421 - acc: 0.9653 - val_loss: 0.1795 - val_acc: 0.9559\n",
            "Epoch 74/100\n",
            "10240/10240 [==============================] - 3s 256us/step - loss: 0.1410 - acc: 0.9648 - val_loss: 0.1780 - val_acc: 0.9551\n",
            "Epoch 75/100\n",
            "10240/10240 [==============================] - 3s 250us/step - loss: 0.1398 - acc: 0.9654 - val_loss: 0.1773 - val_acc: 0.9555\n",
            "Epoch 76/100\n",
            "10240/10240 [==============================] - 3s 256us/step - loss: 0.1393 - acc: 0.9648 - val_loss: 0.1779 - val_acc: 0.9563\n",
            "Epoch 77/100\n",
            "10240/10240 [==============================] - 3s 250us/step - loss: 0.1383 - acc: 0.9658 - val_loss: 0.1764 - val_acc: 0.9555\n",
            "Epoch 78/100\n",
            "10240/10240 [==============================] - 3s 245us/step - loss: 0.1373 - acc: 0.9655 - val_loss: 0.1770 - val_acc: 0.9563\n",
            "Epoch 79/100\n",
            "10240/10240 [==============================] - 2s 236us/step - loss: 0.1363 - acc: 0.9658 - val_loss: 0.1756 - val_acc: 0.9551\n",
            "Epoch 80/100\n",
            "10240/10240 [==============================] - 2s 236us/step - loss: 0.1357 - acc: 0.9663 - val_loss: 0.1753 - val_acc: 0.9559\n",
            "Epoch 81/100\n",
            "10240/10240 [==============================] - 2s 234us/step - loss: 0.1351 - acc: 0.9658 - val_loss: 0.1746 - val_acc: 0.9555\n",
            "Epoch 82/100\n",
            "10240/10240 [==============================] - 2s 236us/step - loss: 0.1343 - acc: 0.9661 - val_loss: 0.1743 - val_acc: 0.9555\n",
            "Epoch 83/100\n",
            "10240/10240 [==============================] - 2s 231us/step - loss: 0.1337 - acc: 0.9657 - val_loss: 0.1744 - val_acc: 0.9559\n",
            "Epoch 84/100\n",
            "10240/10240 [==============================] - 2s 235us/step - loss: 0.1332 - acc: 0.9672 - val_loss: 0.1736 - val_acc: 0.9559\n",
            "Epoch 85/100\n",
            "10240/10240 [==============================] - 2s 237us/step - loss: 0.1322 - acc: 0.9667 - val_loss: 0.1736 - val_acc: 0.9570\n",
            "Epoch 86/100\n",
            "10240/10240 [==============================] - 2s 234us/step - loss: 0.1317 - acc: 0.9666 - val_loss: 0.1729 - val_acc: 0.9563\n",
            "Epoch 87/100\n",
            "10240/10240 [==============================] - 2s 235us/step - loss: 0.1310 - acc: 0.9671 - val_loss: 0.1730 - val_acc: 0.9555\n",
            "Epoch 88/100\n",
            "10240/10240 [==============================] - 2s 236us/step - loss: 0.1305 - acc: 0.9666 - val_loss: 0.1725 - val_acc: 0.9570\n",
            "Epoch 89/100\n",
            "10240/10240 [==============================] - 2s 236us/step - loss: 0.1298 - acc: 0.9668 - val_loss: 0.1724 - val_acc: 0.9566\n",
            "Epoch 90/100\n",
            "10240/10240 [==============================] - 2s 236us/step - loss: 0.1294 - acc: 0.9680 - val_loss: 0.1716 - val_acc: 0.9566\n",
            "Epoch 91/100\n",
            "10240/10240 [==============================] - 2s 233us/step - loss: 0.1287 - acc: 0.9679 - val_loss: 0.1715 - val_acc: 0.9563\n",
            "Epoch 92/100\n",
            "10240/10240 [==============================] - 2s 235us/step - loss: 0.1284 - acc: 0.9681 - val_loss: 0.1710 - val_acc: 0.9566\n",
            "Epoch 93/100\n",
            "10240/10240 [==============================] - 2s 233us/step - loss: 0.1278 - acc: 0.9677 - val_loss: 0.1711 - val_acc: 0.9570\n",
            "Epoch 94/100\n",
            "10240/10240 [==============================] - 2s 234us/step - loss: 0.1272 - acc: 0.9679 - val_loss: 0.1709 - val_acc: 0.9570\n",
            "Epoch 95/100\n",
            "10240/10240 [==============================] - 2s 237us/step - loss: 0.1267 - acc: 0.9683 - val_loss: 0.1705 - val_acc: 0.9566\n",
            "Epoch 96/100\n",
            "10240/10240 [==============================] - 2s 235us/step - loss: 0.1263 - acc: 0.9689 - val_loss: 0.1706 - val_acc: 0.9574\n",
            "Epoch 97/100\n",
            "10240/10240 [==============================] - 2s 233us/step - loss: 0.1257 - acc: 0.9687 - val_loss: 0.1703 - val_acc: 0.9570\n",
            "Epoch 98/100\n",
            "10240/10240 [==============================] - 2s 231us/step - loss: 0.1255 - acc: 0.9688 - val_loss: 0.1702 - val_acc: 0.9574\n",
            "Epoch 99/100\n",
            "10240/10240 [==============================] - 2s 236us/step - loss: 0.1248 - acc: 0.9690 - val_loss: 0.1694 - val_acc: 0.9566\n",
            "Epoch 100/100\n",
            "10240/10240 [==============================] - 2s 233us/step - loss: 0.1242 - acc: 0.9688 - val_loss: 0.1697 - val_acc: 0.9574\n"
          ]
        }
      ],
      "source": [
        "tf_callback = TensorBoard(log_dir=\"logs\", histogram_freq=1)\n",
        "history = model.fit(x_train, \n",
        "                    y_train, \n",
        "                    validation_split=0.2,  # use 20% of the data as the validation set\n",
        "                    callbacks=[EarlyStopping(patience=3),   # If val_loss doesn't decrease after 3 epochs, stop training\n",
        "                               History(),\n",
        "                               tf_callback],\n",
        "                    batch_size=512,  # On every optimization step, we use 512 examples from the training data\n",
        "                    epochs=100)  # 1 epoch = one full pass through the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpJF-CHkLnPJ"
      },
      "source": [
        "We use early stopping as a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-dt-zJLIqfk"
      },
      "source": [
        "**Evaluate the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lXs2fK8IwK8"
      },
      "source": [
        "Once the model has been trained, one should evaluate the model on the held-out test set using different metrics. In our case we use:\n",
        "\n",
        "*   The **auc** metric from **concise.eval_metrics**, it represents the Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n",
        "*   The **aupcr** metric from **concise.eval_metrics**, it rpresents the area under the precision-recall curve (AUPRC) is a useful performance metric for imbalanced data in a problem setting where you care a lot about finding the positive examples.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNfTfKE0LEfl"
      },
      "source": [
        "**Visualize the training history**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0yJVdqPLIPC"
      },
      "source": [
        "As the model sees more and more data, it becomes more accurate on the training set and also on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VgEg5SEMc83-",
        "outputId": "42605f3e-0dca-44fb-8afe-577ac5632c23"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"root\"></div>\n",
              "    <script>\n",
              "      (function() {\n",
              "        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n",
              "        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n",
              "        document.querySelector(\"base\").href = \"https://localhost:6006\";\n",
              "        function fixUpTensorboard(root) {\n",
              "          const tftb = root.querySelector(\"tf-tensorboard\");\n",
              "          // Disable the fragment manipulation behavior in Colab. Not\n",
              "          // only is the behavior not useful (as the iframe's location\n",
              "          // is not visible to the user), it causes TensorBoard's usage\n",
              "          // of `window.replace` to navigate away from the page and to\n",
              "          // the `localhost:<port>` URL specified by the base URI, which\n",
              "          // in turn causes the frame to (likely) crash.\n",
              "          tftb.removeAttribute(\"use-hash\");\n",
              "        }\n",
              "        function executeAllScripts(root) {\n",
              "          // When `script` elements are inserted into the DOM by\n",
              "          // assigning to an element's `innerHTML`, the scripts are not\n",
              "          // executed. Thus, we manually re-insert these scripts so that\n",
              "          // TensorBoard can initialize itself.\n",
              "          for (const script of root.querySelectorAll(\"script\")) {\n",
              "            const newScript = document.createElement(\"script\");\n",
              "            newScript.type = script.type;\n",
              "            newScript.textContent = script.textContent;\n",
              "            root.appendChild(newScript);\n",
              "            script.remove();\n",
              "          }\n",
              "        }\n",
              "        function setHeight(root, height) {\n",
              "          // We set the height dynamically after the TensorBoard UI has\n",
              "          // been initialized. This avoids an intermediate state in\n",
              "          // which the container plus the UI become taller than the\n",
              "          // final width and cause the Colab output frame to be\n",
              "          // permanently resized, eventually leading to an empty\n",
              "          // vertical gap below the TensorBoard UI. It's not clear\n",
              "          // exactly what causes this problematic intermediate state,\n",
              "          // but setting the height late seems to fix it.\n",
              "          root.style.height = `${height}px`;\n",
              "        }\n",
              "        const root = document.getElementById(\"root\");\n",
              "        fetch(\".\")\n",
              "          .then((x) => x.text())\n",
              "          .then((html) => void (root.innerHTML = html))\n",
              "          .then(() => fixUpTensorboard(root))\n",
              "          .then(() => executeAllScripts(root))\n",
              "          .then(() => setHeight(root, 800));\n",
              "      })();\n",
              "    </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir logs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "PFA.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}